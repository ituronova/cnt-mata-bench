#!/usr/bin/env python3
#
# pycobench - a small benchmarking solution
#
# A small environment for running benchmarks
#
# If you need something more elaborate, check, e.g., benchexec
#   ( https://github.com/sosy-lab/benchexec )
#
# Description:
#   * Runs benchmarks in parallel using a parameterized number of workers.
#   * One benchmark is run using different execution engines, specified in a
#     configuration file.
#     * The configuration file is a YAML where each engine is given a command
#       to run (with $1, $2, $3,... denoting input parameters).
#   * Benchmarks are provided on standard input, one per line.  Each line
#     contains a whitespace-separated list of parameters (which are then input
#     into the engine commands in the place of $1, $2, $3, ...)
#   * The tasks to be run are written into a file and results of finished tasks
#     are appended to the file as well.
#   * When interrupted, it is possible to continue executing the unfinished
#     benchmarks from the task list file (parameter -c).
#
# TODO:
#   * better docs
#   * when restarting, check that the tasklist and YAML configuration are compatible
#   * obtain time from the output of the commands
#   * prettier printing of results (csv, html, latex, ...)
#   * functionality as a converter from .tasks file to an output format
#   * check whether the output of the commands matches
#

import argparse
import csv
import os
import queue
import re
import resource
import subprocess
import sys
import threading
import yaml

# thread-safe queue for distributing tasks
g_task_queue = queue.Queue()

# thread-safe queue for collecting results
g_result_queue = queue.Queue()

# a dictionary of commands to run
g_cmd_dict = { }

# timeout for subprocesses (in seconds)
g_timeout = 60

# The file that contains information about submitted and finished tasks.  This
# can be later used for restarting a prematurely stopped benchmark.
g_tasks = 'pycobench.tasks'

# results file
g_results = 'pycobench-results.csv'

# the command to measure CPU time
g_time_cmd = ['/usr/bin/time', '-p']

# the number of tasks that are to be run
g_cnt_tasks = 0

# the number of tasks that have finished
g_cnt_finished_tasks = 0

###########################################
def run_subproc(cmd):
    """run_subproc(cmd) -> dict()

Runs a command as a subprocess and collects results.
"""
    rusage_before = resource.getrusage(resource.RUSAGE_CHILDREN)
    # proc = subprocess.run(cmd, timeout=g_timeout, capture_output=True)
    proc = subprocess.run(cmd, timeout=g_timeout, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    rusage_after = resource.getrusage(resource.RUSAGE_CHILDREN)
    result = {}
    result['retcode'] = proc.returncode
    result['stdout'] = proc.stdout.decode().strip()
    result['stderr'] = proc.stderr.decode().strip()

    result['time'] = rusage_after.ru_utime - rusage_before.ru_utime
    return result


###########################################
def run_subproc_systime(cmd):
    """run_subproc(cmd) -> dict()

Runs a command as a subprocess and collects results.
"""
    cmd = g_time_cmd + cmd
    # proc = subprocess.run(cmd, timeout=g_timeout, capture_output=True)
    proc = subprocess.run(cmd, timeout=g_timeout, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    result = {}
    result['retcode'] = proc.returncode
    result['stdout'] = proc.stdout.decode().strip()
    result['stderr'] = proc.stderr.decode().strip()

    stderr_split = result['stderr'].splitlines()
    for line in stderr_split:
        mtch = re.search(r'user\s+(?P<time>\d+\.\d+)', line)
        if mtch:
            result['time'] = float(mtch['time'])

    if 'time' not in result:
        raise Exception("Could not extract measured time")

    return result


###########################################
def execute_benchmark(params):
    """execute_benchmark(params) -> None

Executes one benchmark.
"""
    global g_cmd_dict
    name = params['method']
    in_params = params['params']
    cmd = g_cmd_dict[name]['cmd']
    cmd = cmd.split()
    # in_params = in_params.split()

    # substitute $1, $2, etc. with the real parameters
    for i in range(len(cmd)):
        if len(cmd[i]) == 2 and cmd[i][0] == '$':
            try:
                x = int(cmd[i][1])
            except:
                raise Exception('invalid placeholder \"' + cmd[i] + '\" in the command to run')

            if x > len(in_params):
                raise Exception('parameter ' + cmd[i] + ' not provided (only ' +
                    str(len(in_params)) + ' parameters passed)')
            else:
                cmd[i] = in_params[x-1]

    try:
        # result = run_subproc(cmd)
        result = run_subproc_systime(cmd)
        return result
    except subprocess.TimeoutExpired:
        return {'timeout': True}

###########################################
def merge_two_dicts(x, y):
    z = x.copy()   # start with x's keys and values
    z.update(y)    # modifies z with y's keys and values & returns None
    return z

###########################################
def worker():
    """worker() -> None

Main function of a thread for processing tasks.
"""
    while True:
        item = g_task_queue.get()
        if item is None:     # None signals end of processing
            g_result_queue.put(None)   # signal termination of worker
            break
        res = execute_benchmark(item)
        g_result_queue.put(merge_two_dicts(item, res))
        g_task_queue.task_done()


###########################################
def process_result(writer, task_file, result):
    """process_result(writer, task_file, result) -> None

Processes one obtained result (writes it using writer [and flushes, as a good
christian]).
"""
    res_string = "UNKNOWN";
    if 'timeout' in result:
        writer.writerow(['timeout', result['method'], result['params']])
        task_file.flush()
        res_string = 'TIMEOUT'
    else:
        str_stdout = result['stdout'].replace('\n','###')
        str_stderr = result['stderr'].replace('\n','###')
        status = 'finished'
        res_string = 'FINISHED \tResult: {}\tTime: {}'.format(result['retcode'], result['time'])

        writer.writerow([status, result['method'], result['params'],
            result['retcode'], str_stdout, str_stderr,
            result['time']])
        task_file.flush()

    global g_cnt_finished_tasks
    g_cnt_finished_tasks += 1
    print(str("{}/{}\t{}\t{}:\t{}".format(
        g_cnt_finished_tasks,
        g_cnt_tasks,
        result['method'],
        result['params'],
        res_string)))


###########################################
def create_writer(opened_file):
    """create_writer(opened_file) -> csv.Writer """
    writer = csv.writer(
        opened_file, delimiter=';', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    return writer


###########################################
def process_conf_file(conf_file):
    """process_conf_file(conf_file) -> None

Processes the configuration file.
"""
    global g_cmd_dict
    config = yaml.load(conf_file, Loader=yaml.FullLoader)
    g_cmd_dict = config


###########################################
def prepare_list_of_tasks_from_tasklist(tasklist_filename):
    """prepare_list_of_tasks_from_tasklist(str tasklist_filename) -> list

Checkes the file 'tasklist_filename' and extracts from it tasks that have not
been finished yet.  These are then returned in a list.
"""
    executed_tasks = set()
    finished_tasks = set()
    with open(tasklist_filename, 'r') as tasklist_file:
        reader = csv.reader(
            tasklist_file, delimiter=';', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        try:
            for row in reader:
                # create two sets
                if row[0] == 'execute':
                    executed_tasks.add((row[1], row[2]))
                elif row[0] == 'finished':
                    finished_tasks.add((row[1], row[2]))
        except Exception as ex:
            print('Error reading a task list at line ' + str(reader.line_num) + ': '
                + str(ex))
            sys.exit(1)

    unfinished_business = executed_tasks - finished_tasks
    list_of_tasks = [{'method': x, 'params': y} for (x,y) in unfinished_business]

    return list_of_tasks


###########################################
def run_main(args):
    """run_main(args) -> None

Runs the main program according to the arguments obtained from the parser.
"""
    assert(len(args.conf) == 1)
    with open(args.conf[0], 'r') as conf_file:
        process_conf_file(conf_file)

    list_of_tasks = []   # this is where we keep the tasks that are to be procecessed
    if args.tasklist:   # we want to continue in a tasklist
        list_of_tasks = prepare_list_of_tasks_from_tasklist(args.tasklist)
    else:  # take the tasks from stdin
        # processing the input
        if False:  # changed to CSV reader
            for line in sys.stdin:
                line = line.rstrip()
                for k in g_cmd_dict:
                    list_of_tasks.append({'method': k, 'params': line})
        reader = csv.reader(sys.stdin, delimiter=';')
        for line in reader:
            for k in g_cmd_dict:
                list_of_tasks.append({'method': k, 'params': line})

        # write into task_file what we're executing
        with open(g_tasks, 'w') as task_file:
            writer = create_writer(task_file)
            for task in list_of_tasks:
                writer.writerow(['execute', task['method'], task['params']])

    global g_cnt_tasks
    g_cnt_tasks = len(list_of_tasks)

    # set the number of workers
    num_worker_threads = args.jobs
    if num_worker_threads is None:
        num_worker_threads = 1

    # start the workers
    threads = []
    for i in range(num_worker_threads):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)

    # queue the tasks
    for task in list_of_tasks:
        g_task_queue.put(task)

    # send the END OF TASKS message
    for t in threads:
        g_task_queue.put(None)

    with open(g_tasks, 'a') as task_file:
        writer = create_writer(task_file)

        # processing the results
        finished_workers = 0
        while finished_workers < len(threads):
            result = g_result_queue.get()
            if result is None:
                print("worker terminated")
                finished_workers += 1
                continue
            else:
                process_result(writer, task_file, result)

    # a barrier
    for t in threads:
        t.join()

###########################################
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='''pycobench - \
Executes benchmarks given using a configuration file on cases given on the \
standard input''')
    parser.add_argument('conf', metavar='file.conf', nargs=1,
        help='configuration file (in YAML)')
    parser.add_argument('-j', '--jobs', type=int, default = os.cpu_count(),
        help='The number of jobs (workers) to run concurrently (default: %(default)s)')
    parser.add_argument('-c', '--continue', metavar='TASKLIST', dest='tasklist',
        help='''Specifying this argument continues execution of unfinished \
tasks from %(metavar)s.  No input is read.''')

    args = parser.parse_args()
    run_main(args)
